{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO \n",
    "UwU\n",
    "\n",
    "<p style=\"font-size: 17px;margin:0\">PPO is widely used for its stability. Let's make one!<p>\n",
    "<p style=\"font-size: 17px;margin:0\">Idea is instead of E[log pi(a | s) * advantage] </p>\n",
    "<p style=\"font-size: 17px;margin:0\">We use E[new_pi(a|s)/old_pi(a|s) * advantage]</p>\n",
    "\n",
    "<p style=\"font-size: 17px\">This is the same advantage as we use for actor critic (r + y * V(next state) - V(current state))</p>\n",
    "<p style=\"font-size: 17px;margin:0\">However we will change up this advantage function by using Generative Advantage Estimation (GAE) instead. This is optional but highly recommended. Its implementation is already filled out for you</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything we need\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where we make the PPO class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-204e3ee7b4cb>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-204e3ee7b4cb>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    pi_probs, self.v_pred, pi_params = # YOUR CODE HERE build new network\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PPO:\n",
    "    # takes in state dimension, action dimension, learning rate, gamma, and clip value\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma, clip_val):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.clip_val = clip_val\n",
    "        \n",
    "        # Here we will make the placeholders. Make sure to give these placeholders names!\n",
    "        # Besides the action being of type tf.int32, everything has type tf.float32\n",
    "        # Make a state placeholder. It should have shape [None, state_dim]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Make an action placeholder. It should have shape [None, ] and be of type tf.int32\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Make a reward placeholder. It should have shape [None, ]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Make a advantage placeholder. It should have shape [None, ]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Make a next value placeholder. It should have shape [None, ]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        pi_probs, self.v_pred, pi_params = # YOUR CODE HERE build new network\n",
    "        oldpi_probs, _, oldpi_params = # YOUR CODE HERE build old network\n",
    "        \n",
    "        # Use tf.multinomial to select a random action based on our new policy's probability distribution\n",
    "        # Since this returns a 1 by 1 array we need to use tf.squeeze to force it into a scalar value\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # sets parameters of the old policy to equal to the parameters of the new policy\n",
    "        self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "        \n",
    "        # We only need the probability of the action we actually took\n",
    "        # Create a one hot encoding of our actions (action placeholder)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Determine probability of the action of our new policy and old policy\n",
    "        # Use tf.reduce_sum of axis 1 to compress down our probabilities\n",
    "        act_prob = # YOUR CODE HERE\n",
    "        old_act_prob = # YOUR CODE HERE\n",
    "        \n",
    "        # Determine critic loss\n",
    "        with tf.variable_scope('critic_loss'):\n",
    "            # Our critic loss is still same as our actor critic's critic loss\n",
    "            # reward + gamma * next value (use the placeholder) - current value\n",
    "            self.critic_loss = tf.reduce_mean(\n",
    "                # YOUR CODE HERE\n",
    "            );\n",
    "        \n",
    "        with tf.variable_scope('actor_loss'):\n",
    "            # Our actor loss is ratio of our new actor's prob and old actor's prob multiplied by advantage\n",
    "            ratio = # YOUR CODE HERE just calculate ratio for now\n",
    "            clipped_ratio = # YOUR CODE HERE clip our ratio with tf.clip_by_value\n",
    "            \n",
    "            self.actor_loss = tf.reduce_mean(\n",
    "                # YOUR CODE HERE\n",
    "            );\n",
    "        \n",
    "        with tf.variable_scope('total_loss'):\n",
    "            self.total_loss = # YOUR CODE HERE find total loss\n",
    "            optimizer = # YOUR CODE HERE use adam optimizer\n",
    "            self.train_op = # YOUR CODE HERE minimize total loss (IMPORTANT! var_list = pi_params)\n",
    "            # We only want to train our new policy. Don't update our old policy!!!!     \n",
    "    \n",
    "    # Builds the network\n",
    "    def _build_network(self, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Creates policy network\n",
    "            with tf.variable_scope('policy'): \n",
    "                # Create a 4 layer network (two hidden layers each with 40 neurons and \n",
    "                # output layer with number of neurons equal to number of possible actions)\n",
    "                # Also create a variable named probs equal to the softmax probability of your output layer\n",
    "                # YOUR CODE HERE\n",
    "            \n",
    "            # Create critic network\n",
    "            with tf.variable_scope('critic'):\n",
    "                # Create a 4 layer network (two hidden layers each with 40 neurons and \n",
    "                # output layer with number of neurons equal to 1)\n",
    "                # Also create a variable named v_pred equal to your output layer\n",
    "                # YOUR CODE HERE\n",
    "        \n",
    "        # Grab all of the trainable variables of both policy and critic networks within our scope\n",
    "        # We will need this to update our old policy parameters\n",
    "        # Look into tf.get_collection. Remeber to use scope!\n",
    "        params = # YOUR CODE HERE\n",
    "        return probs, v_pred, params\n",
    "    \n",
    "    def get_action(self, sess, state):\n",
    "        return # return random action\n",
    "\n",
    "    def get_value(self, sess, state):\n",
    "        return # return value of state\n",
    "\n",
    "    def update_params(self, sess):\n",
    "        # update parameters of old network\n",
    "\n",
    "    def train(self, sess, states, actions, rewards, advantages, next_vs):\n",
    "        feed_dict = # Create dictionary of placeholders : values\n",
    "        total_loss, actor_loss, critic_loss, _ = # Get total loss, actor loss, critic loss, and run train op\n",
    "        return total_loss, actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaes(gamma, rewards, v_preds, v_preds_next):\n",
    "    # Code is in GAE file.\n",
    "    # Huge props if you make it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Cartpole-v0')\n",
    "    env.seed(0)\n",
    "    gamma = 0.99\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Create agent and initialize variables\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        for iteration in range(100000):\n",
    "            states = []\n",
    "            actions = []\n",
    "            v_preds = []\n",
    "            rewards = []\n",
    "            total_reward = 0\n",
    "            \n",
    "            obs = env.reset()\n",
    "            while True:\n",
    "                action = # YOUR CODE HERE get agent's action. Use [np.newaxis, :] to add a dimension to state\n",
    "                value = # YOUR CODE HERE get value of the state. \n",
    "                # The value will be a 1 by 1 list but we need the scalar value\n",
    "                \n",
    "                new_obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                tates.append(obs)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                v_preds.append(value)\n",
    "\n",
    "                obs = new_obs\n",
    "\n",
    "                if done:\n",
    "                    # The value list goes from the first state to last state\n",
    "                    # However we want the values from the second state to last state + [0]\n",
    "                    # We need values of next states not current states\n",
    "                    v_preds_next =  # YOUR CODE HERE\n",
    "                    print(sum(rewards))\n",
    "                    break\n",
    "                    \n",
    "            gaes = np.array(get_gaes(gamma, rewards, v_preds, v_preds_next))\n",
    "            gaes = (gaes - gaes.mean()) / gaes.std() # normalize advantages for stability\n",
    "            \n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions).astype(dtype=np.int32)\n",
    "            rewards = np.array(rewards).astype(dtype=np.float32)\n",
    "            v_preds_next = np.array(v_preds_next).astype(dtype=np.float32)\n",
    "            \n",
    "            # Update our old network's parameters\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            inp = [states, actions, rewards, v_preds_next, gaes]\n",
    "            \n",
    "            # Idea is we train it four times while clipping values\n",
    "            for epoch in range(4):\n",
    "                sample_indices = np.random.randint(low=0, high=states.shape[0], size=64)  # indices are in [low, high)\n",
    "                sampled_inp = [np.take(a=a, indices=sample_indices, axis=0) for a in inp]  # sample training data\n",
    "                agent.train(sess, sampled_inp[0], sampled_inp[1], sampled_inp[2], \n",
    "                            sampled_inp[3], sampled_inp[4])\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
